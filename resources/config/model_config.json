{
    "models": [
        {
            "model_id": "chatglm3",
            "name": "ZhipuAI/chatglm3-6b",
            "promulgator": "智谱AI和清华大学 KEG 实验室联合发布",
            "date": "2023年10月27日",
            "country": "国内",
            "parameters_size": "6B",
            "training_tokens": "1T",
            "max_length": "2K",
            "commercial_use": "可商用",
            "url": "https://github.com/THUDM/ChatGLM3",
            "description": "ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，拥有更强大的基础模型、更完整的功能支持，包括正常的多轮对话外，支持工具调用、代码执行等复杂场景。"
        },
        {
            "model_id": "chatglm2",
            "name": "ZhipuAI/chatglm2-6b",
            "promulgator": "智谱AI和清华大学 KEG 实验室联合发布",
            "date": "2023年6月25日",
            "country": "国内",
            "parameters_size": "6B",
            "training_tokens": "1.4T",
            "max_length": "32K",
            "commercial_use": "可商用",
            "url": "https://github.com/THUDM/ChatGLM2-6B",
            "description": "ChatGLM2-6B 是 ChatGLM-6B 的第二代版本，引入了混合目标函数，扩展了上下文长度，实现了更高效的推理速度和更低的显存占用。"
        },
        {
            "model_id": "baichuan2-chat",
            "name": "baichuan-inc/Baichuan2-7B-Chat",
            "promulgator": "百川智能",
            "date": "2023年9月6日",
            "country": "国内",
            "parameters_size": "7B",
            "training_tokens": "1.2T/1.4T",
            "max_length": "4K",
            "commercial_use": "可商用",
            "url": "https://github.com/baichuan-inc/Baichuan2",
            "description": "Baichuan2-7B-Chat 是 Baichuan 系列的一个版本，专注于中文领域的对话和问答，支持4K上下文长度。"
        },
        {
            "model_id": "qwen-7b-chat",
            "name": "qwen/Qwen-7B-Chat",
            "promulgator": "阿里云",
            "date": "2023年8月3日",
            "country": "国内",
            "parameters_size": "7B",
            "training_tokens": "2.2T",
            "max_length": "8K",
            "commercial_use": "可商用",
            "url": "https://github.com/QwenLM/Qwen",
            "description": "Qwen-7B-Chat 是基于7B参数的模型，针对多轮对话优化，支持长上下文和插件调用。"
        },
        {
            "model_id": "internlm-chat",
            "name": "AI-ModelScope/internlm-chat-7b",
            "promulgator": "上海人工智能实验室（上海AI实验室）",
            "date": "2023年8月21日",
            "country": "国内",
            "parameters_size": "7B",
            "training_tokens": "1.6T",
            "max_length": "2K",
            "commercial_use": "可商用",
            "url": "https://github.com/InternLM/InternLM",
            "description": "InternLM-Chat-7B 是一个中文优化的模型，支持多轮对话和各种语言处理任务。"
        },
        {
            "model_id": "yi_6b_chat",
            "name": "01ai/Yi-6B-Chat",
            "promulgator": "零一万物",
            "date": "2023年11月24日",
            "country": "国内",
            "parameters_size": "6B",
            "training_tokens": "3T",
            "max_length": "2K",
            "commercial_use": "可商用",
            "url": "https://github.com/01-ai/Yi",
            "description": "Yi-6B-Chat 是一个中文对话模型，支持复杂的对话场景和流畅的语言生成。"
        },
        {
            "model_id": "Yi-34b-chat",
            "name": "01ai/Yi-34B-Chat",
            "promulgator": "零一万物",
            "date": "2023年11月24日",
            "country": "国内",
            "parameters_size": "34B",
            "training_tokens": "3T",
            "max_length": "4K",
            "commercial_use": "可商用",
            "url": "https://github.com/01-ai/Yi",
            "description": "Yi-34B是由零一万物开发并开源的双语大语言模型，该版本为支持对话的chat版本。"
        },
        {
            "model_id": "mistral-7b-openorca",
            "name": "AI-ModelScope/Mistral-7B-Instruct-v0.2",
            "promulgator": "Mistral AI",
            "date": "2023年9月27日",
            "country": "国外",
            "parameters_size": "7B",
            "training_tokens": "8T",
            "max_length": "8K",
            "commercial_use": "可商用",
            "url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
            "description": "Mistral-7B-Instruct-v0.2 Large Language Model (LLM) 是 Mistral-7B-Instruct-v0.1 的改进微调版本。"
        },
        {
            "model_id": "stable-vicuna",
            "name": "AI-ModelScope/Vicuna-7B",
            "promulgator": "lmsys",
            "date": "2023年03月31日",
            "country": "国外",
            "parameters_size": "7B",
            "training_tokens": "125K",
            "max_length": "2k",
            "commercial_use": "不可商用",
            "url": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "description": "Vicuna 是通过对 Llama 2 进行微调，并根据 ShareGPT 收集到的用户共享对话训练出来的聊天助手。"
        },
        {
            "model_id": "baichuan2-chat",
            "name": "baichuan-inc/Baichuan2-13B-Chat",
            "promulgator": "百川智能",
            "date": "2023年12月29日",
            "country": "国内",
            "parameters_size": "13B",
            "training_tokens": "2.6T",
            "max_length": "4K",
            "commercial_use": "可商用",
            "url": "https://github.com/baichuan-inc/Baichuan2-13B?tab=readme-ov-file",
            "description": "Baichuan2-13B-Chat为Baichuan-13B系列模型中对齐后的版本。"
        },
        {
            "model_id": "llama-2",
            "name": "modelscope/Llama-2-7b-chat-ms",
            "promulgator": "Meta",
            "date": "2023年7月18日",
            "country": "国外",
            "parameters_size": "7B",
            "training_tokens": "2T",
            "max_length": "4K",
            "commercial_use": "可商用",
            "url": "https://ai.meta.com/resources/models-and-libraries/llama/",
            "description": "Llama-2-Chat 是在Llama-2上经过微调的大语言模型，并针对对话用例进行了优化。 "
        },
        {
            "model_id": "llama-2",
            "name": "modelscope/Llama-2-13b-chat-ms",
            "promulgator": "Meta",
            "date": "2023年7月18日",
            "country": "国外",
            "parameters_size": "13B",
            "training_tokens": "2T",
            "max_length": "4K",
            "commercial_use": "可商用",
            "url": "https://ai.meta.com/resources/models-and-libraries/llama/",
            "description": "Llama-2-Chat 是在Llama-2上经过微调的大语言模型，并针对对话用例进行了优化。"
        },
        {
            "model_id": "openchat_3.5",
            "name": "myxiongmodel/openchat_3.5",
            "promulgator": "imoneoi",
            "date": "2023年11月01日",
            "country": "国外",
            "parameters_size": "7B",
            "training_tokens": "",
            "max_length": "8K",
            "commercial_use": "可商用",
            "url": "https://github.com/imoneoi/openchat",
            "description": "OpenChat 是一个创新的开源语言模型库，采用 C-RLFT 进行微调，这种策略的灵感来自离线强化学习。"
        },
        {
            "model_id": "qwen-7b-chat",
            "name": "qwen/Qwen-14B-Chat",
            "promulgator": "阿里云",
            "date": "2023年9月25日",
            "country": "国内",
            "parameters_size": "14B",
            "training_tokens": "3T",
            "max_length": "2K",
            "commercial_use": "可商用",
            "url": "https://github.com/QwenLM/Qwen",
            "description": "通义千问-14B（Qwen-14B）是阿里云研发的通义千问大模型系列的140亿参数规模的模型。在Qwen-14B的基础上，阿里云使用对齐机制打造了基于大语言模型的AI助手Qwen-14B-Chat。"
        }
    ]
}